* Project Title: GPT1-From-Scratch: Generative Pretrained Transformer Model Development
  
*  Designed and developed a decoder-based Generative Pretrained Transformer (GPT) model from scratch, achieving a 78% reduction in training loss (from 4.37 to 0.94) through the implementation of multi-headed attention, positional encoding, and dropout with layer normalization.
  
*  Demonstrated state-of-the-art performance in natural language understanding and generation, with a 79% reduction in inference loss (from 5.21 to 1.08) on unseen data, showcasing improved model generalizability and robustness.
